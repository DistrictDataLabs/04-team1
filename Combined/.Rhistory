p=0.7,
list=FALSE)
validation = Wage[-inBuild,];
buildData = Wage[inBuild,]
inTrain = createDataPartition(y=buildData$wage,
p=0.7,
list=FALSE)
training = buildData[inTrain,]
testing = buildData[-inTrain,]
library(ISLR)
data(Wage)
library(ggplot2)
library(caret)
Wage = subset(Wage, select=-c(logwage))
#Create a building data set and validation set
inBuild = createDataPartition(y=Wage$wage,
p=0.7,
list=FALSE)
validation = Wage[-inBuild,];
buildData = Wage[inBuild,]
inTrain = createDataPartition(y=buildData$wage,
p=0.7,
list=FALSE)
training = buildData[inTrain,]
testing = buildData[-inTrain,]
dim(training)
dim(testing)
dim(validation)
mod1 = train(wage ~., method="glm", data=training)
mod2 = train(wage ~., method="rf",
data=training,
trControl=trainControl(method="cv"),
number=3)
mod2 = train(wage ~., method="rf",
data=training,
trControl=trainControl(method="cv"),
number=3)
mod2
mod2 = train(wage ~., method="rf",
data=training,
trControl=trainControl(method="cv"),
number=3)
pred1 = predict(mod1,testing)
pred2 = predict(mod2,testing)
qplot(pred1,pred2,colour=wage,data=testing)
pred1
predDF = data.fram(pred1,pred2,wage=testing$wage)
combModFit = train(wage ~.,
method="gam",
data=predDF)
combPred = predict(combModFit,predDF)
predDF = data.frame(pred1,pred2,wage=testing$wage)
combModFit = train(wage ~.,
method="gam",
data=predDF)
combPred = predict(combModFit,predDF)
predDF
combPred
sqrt(sum(pred1-testing$Wage)^2)
sqrt(sum(pred2-testing$Wage)^2)
sqrt(sum(combPred-testing$Wage)^2)
sqrt(sum(pred1-testing$wage)^2)
sqrt(sum(pred2-testing$wage)^2)
sqrt(sum(combPred-testing$wage)^2)
predDF
combPredDF = data.frame(pred1,pred2,combPred,wage=testing$wage)
combPredDf
combPredDF
head(combPredDF)
combPredDF
sqrt(sum(combPred-testing$wage)^2)
combPred-testing$wage
sum(combPred-testing$wage)
sum(combPred-testing$wage)^2
sqrt(sum(combPred-testing$wage)^2)
sqrt(sum((pred1-testing$wage)^2))
sqrt(sum((pred2-testing$wage)^2))
sqrt(sum((combPred-testing$wage)^2))
pred1V = predict(mod1, validation)
pred2V = predict(mod2, validation)
predVDF = data.frame(pred1=pred1V, pred2=pred2V)
combPredV = predict(combModFit,predVDF)
combPredV
sqrt(sum((pred1V-validation$wage)^2))
sqrt(sum((pred2V-validation$wage)^2))
sqrt(sum((combPredV-validation$wage)^2))
library(quantmod)
from.dat = as.Date("01/01/08", format="%m/%d/%y")
to.dat = as.Date("12/31/13", format="%m/%d/%y")
getSymbols("GOOG",
src="google",
from=from.dat,
to=to.date)
getSymbols("GOOG",
src="google",
from=from.dat,
to=to.dat)
GOOG
head(GOOG)
mGoog = to.monthly(GOOG)
googOpen = Op(mGoog)
ts1 = ts(googOpen, frequency=12)
plot(ts1,xlab="Years+1", ylab="GOOG")
mGoog = to.monthly(GOOG)
GOOGm = GOOG[-GOOG.Volume]
GOOGm = GOOG[-GOOG$GOOG.Volume]
GOOGm = -GOOG$GOOG.Volume
getSymbols("GOOG",
src="yahoo",
from=from.dat,
to=to.dat)
getSymbols("GOOGL",
src="yahoo",
from=from.dat,
to=to.dat)
GOOGL
head(GOOGL)
mGoog = to.monthly(GOOGL)
googOpen = Op(mGoog)
ts1 = ts(googOpen, frequency=12)
plot(ts1,xlab="Years+1", ylab="GOOGL")
plot(decompose(ts1), xlab="Years+1")
ts1Train = window(ts1, start=1, end=5)
ts1Test = window(ts1, start=5, end=(7-0.01))
ts1Train
ts1Test
plot(ts1Train)
lines(ma(ts1Train, order=3), col="red")
library(ts)
install.packages("ts")
help.find(ma)
library(TTR)
plot(ts1Train)
lines(ma(ts1Train, order=3), col="red")
library(forecast)
plot(ts1Train)
lines(ma(ts1Train, order=3), col="red")
install.packages("forecaset")
install.packages("forecast")
library(forecast)
plot(ts1Train)
lines(ma(ts1Train, order=3), col="red")
ets1 = ets(ts1Train, model="MMM")
fcast = forecast(ets1)
plot(fcast)
lines(ts1Test,col="red")
accuracy(fcast,ts1Test)
install.packages("quandl")
install.packages("Quandl")
data(iris)
library(ggplot2)
inTrain = createDataPartition(y=iris$Species,
p=0.7,
list=FALSE)
training = iris[inTrain,]
testing = iris[-inTrain,]
dim(training)
dim(testing)
kMeans1 = kmeans(subset(training,select=-c(Species)),centers=3)
training$clusters = as.factor(kMeans1$cluster)
qplot(Petal.Width,Petal.Length,colour=clusters,data=training)
table(kMeans1$cluster,training$Species)
modFit = train(clusters ~., data=subset(training, select = -c(Species)), method="rpart")
table(predict(modFit, training), training$Species)
testClusterPred = predict(modFit, testing)
table(testClusterPred, testing$Species)
install.packages("RCurl")
install.packages("jsonlite")
install.packages("plyr")
install.packages("quantmod")
library(RCurl)
> library(jsonlite)
> library(plyr)
pwd
getwd()
library(RCurl)
library(jsonlite)
library(plyr)
library(quantmod)
fixJSON <- function(json)
{
gsub('([^,{:]+):', '"\1":', json)
}
URL1 = 'http://www.google.com/finance/option_chain?q=%s&output=json'
URL2 = 'http://www.google.com/finance/option_chain?q=%s&output=json&expy=%d&expm=%d&expd=%d'
getOptionQuotes <- function(symbol)
{
url     = sprintf(URL1, symbol)
chain   = fromJSON(fixJSON(getURL(url)))
options = mlply(chain$expirations, function(y, m, d)
{
url                     = sprintf(URL2, symbol, y, m, d)
expiry                  = fromJSON(fixJSON(getURL(url)))
expiry$calls$type       = "Call"
expiry$puts$type        = "Put"
prices                  = rbind(expiry$calls, expiry$puts)
prices$expiry           = sprintf("%4d-%02d-%02d", y, m, d)
prices$underlying.price = expiry$underlying_price
prices
})
options         = cbind(data.frame(symbol), rbind.fill(options))
names(options)1 = c("price", "bid", "ask", "open.interest")
for (col in c("strike", "price", "bid", "ask")) options[, col] = as.numeric(options[, col])
options[, "open.interest"] = suppressWarnings(as.integer(options[, "open.interest"]))
options[, c(1, 16, 15, 6, 10, 11, 17, 14, 12)]
}
AAPL = getOptionQuotes("AAPL")
nrow(AAPL)
head(AAPL)
tail(AAPL)
getOptionQuotes <- function(symbol)
{
url     = sprintf(URL1, symbol)
chain   = fromJSON(fixJSON(getURL(url)))
options = mlply(chain$expirations, function(y, m, d)
{
url                     = sprintf(URL2, symbol, y, m, d)
expiry                  = fromJSON(fixJSON(getURL(url)))
expiry$calls$type       = "Call"
expiry$puts$type        = "Put"
prices                  = rbind(expiry$calls, expiry$puts)
prices$expiry           = sprintf("%4d-%02d-%02d", y, m, d)
prices$underlying.price = expiry$underlying_price
prices
})
options         = cbind(data.frame(symbol), rbind.fill(options))
names(options)  = c("price", "bid", "ask", "open.interest")
for (col in c("strike", "price", "bid", "ask")) options[, col] = as.numeric(options[, col])
options[, "open.interest"] = suppressWarnings(as.integer(options[, "open.interest"]))
options[, c(1, 16, 15, 6, 10, 11, 17, 14, 12)]
}
AAPL = getOptionQuotes("AAPL")
fixJSON <- function(json)
{
gsub('([^,{:]+):', '"\1":', json)
}
URL1 = 'http://www.google.com/finance/option_chain?q=%s&output=json'
URL2 = 'http://www.google.com/finance/option_chain?q=%s&output=json&expy=%d&expm=%d&expd=%d'
url
getOptionQuotes <- function(symbol)
{
url     = sprintf(URL1, symbol)
chain   = fromJSON(fixJSON(getURL(url)))
options = mlply(chain$expirations, function(y, m, d)
{
url                     = sprintf(URL2, symbol, y, m, d)
expiry                  = fromJSON(fixJSON(getURL(url)))
expiry$calls$type       = "Call"
expiry$puts$type        = "Put"
prices                  = rbind(expiry$calls, expiry$puts)
prices$expiry           = sprintf("%4d-%02d-%02d", y, m, d)
prices$underlying.price = expiry$underlying_price
prices
})
options           = cbind(data.frame(symbol), rbind.fill(options))
names(options)1   = c("price", "bid", "ask", "open.interest")
for (col in c("strike", "price", "bid", "ask")) options[, col] = as.numeric(options[, col])
options[, "open.interest"] = suppressWarnings(as.integer(options[, "open.interest"]))
options[, c(1, 16, 15, 6, 10, 11, 17, 14, 12)]
}
getOptionQuotes <- function(symbol)
{
url     = sprintf(URL1, symbol)
chain   = fromJSON(fixJSON(getURL(url)))
options = mlply(chain$expirations, function(y, m, d)
{
url                     = sprintf(URL2, symbol, y, m, d)
expiry                  = fromJSON(fixJSON(getURL(url)))
expiry$calls$type       = "Call"
expiry$puts$type        = "Put"
prices                  = rbind(expiry$calls, expiry$puts)
prices$expiry           = sprintf("%4d-%02d-%02d", y, m, d)
prices$underlying.price = expiry$underlying_price
prices
})
options           = cbind(data.frame(symbol), rbind.fill(options))
names(options)1   = c("price", "bid", "ask", "open.interest")
for (col in c("strike", "price", "bid", "ask")) options[, col] = as.numeric(options[, col])
options[, "open.interest"] = suppressWarnings(as.integer(options[, "open.interest"]))
options[, c(1, 16, 15, 6, 10, 11, 17, 14, 12)]
}
AAPL = getOptionQuotes("AAPL")
getOptionQuotes <- function(symbol)
{
url     = sprintf(URL1, symbol)
chain   = fromJSON(fixJSON(getURL(url)))
options = mlply(chain$expirations, function(y, m, d)
{
url                     = sprintf(URL2, symbol, y, m, d)
expiry                  = fromJSON(fixJSON(getURL(url)))
expiry$calls$type       = "Call"
expiry$puts$type        = "Put"
prices                  = rbind(expiry$calls, expiry$puts)
prices$expiry           = sprintf("%4d-%02d-%02d", y, m, d)
prices$underlying.price = expiry$underlying_price
prices
})
options           = cbind(data.frame(symbol), rbind.fill(options))
names(options)1   = c("price", "bid", "ask", "open.interest")
for (col in c("strike", "price", "bid", "ask")) options[, col] = as.numeric(options[, col])
options[, "open.interest"] = suppressWarnings(as.integer(options[, "open.interest"]))
options[, c(1, 16, 15, 6, 10, 11, 17, 14, 12)]
}
AAPL = getOptionQuotes("AAPL")
expiry$puts$type        = "Put"
urlTest = sprintf(URL1, "AAPL")
urlTest
chainTest = fromJSON(fixJSON(getURL(urlTest)))
chainTest
help.search("mlply")
head(chain$expirations)
head(chainTest$expirations)
chainTest
urlTest
jsonTest = fixJSON(getURL(urlTest))
jsonTest
head(jsonTest)
help.search("gsub")
fixJSON <- function(json)
{
gsub('([^,{:]+):', '"\":', json)
}
jsonTest = fixJSON(getURL(urlTest))
jsonTest
help.search("fromJSON")
chainTest = fromJSON(getURL(urlTest))
chainTest
urlTest
install.packages("quantstrat")
data.dir   <- 'Users/Copper/Documents/Prema/Prema_Folder/Kaggle/Facial_Keypoints_Detection'
train.file <- paste0(data.dir, 'training.csv')
test.file  <- paste0(data.dir, 'test.csv')
head(train.file)
train.file
d.train <- read.csv(train.file, stringsAsFactors=F)
data.dir   <- 'Users/Copper/Documents/Prema/Prema_Folder/Kaggle/Facial_Keypoints_Detection/'
train.file <- paste0(data.dir, 'training.csv')
test.file  <- paste0(data.dir, 'test.csv')
d.train <- read.csv(train.file, stringsAsFactors=F)
data.dir   <- '/Users/Copper/Documents/Prema/Prema_Folder/Kaggle/Facial_Keypoints_Detection/'
train.file <- paste0(data.dir, 'training.csv')
test.file  <- paste0(data.dir, 'test.csv')
d.train <- read.csv(train.file, stringsAsFactors=F)
head(d.train)
str(d.train)
im.train      = d.train$Image
d.train$Image = NULL
head(d.train)
im.train[1]
as.integer(unlist(strsplit(im.train[1], " ")))
im.train.test = as.integer(unlist(strsplit(im.train[1], " ")))
ncol(im.train.test)
nrow(im.train.test)
class(im.train.test)
install.packages('doMC')
install.packages("doMC")
library(doMC)
registerDoMC()
im.train <- foreach(im = im.train, .combine=rbind) %dopar%
{
as.integer(unlist(strsplit(im, " ")))
}
str(im.train)
str(test.csv)
d.test  = read.csv(test.file, stringsAsFactors=F)
str(d.test)
head(d.test)
#crate a new field in the test data
im.test      = d.test$Image
d.test$Image = NULL
im.test
head(im.test)
im.test <- foreach(im = im.test, .combine=rbind) %dopar%
{
as.integer(unlist(strsplit(im, " ")))
}
str(im.test)
save(d.train, im.train, d.test, im.test, file='fkd.Rd')
im = matrix(data=rev(im.train[1,]), nrow=96, ncol=96)
image(1:96, 1:96, im, col=gray((0:255)/255))
im_rev <- matrix(data=im.train[1,], nrow=96, ncol=96)
load('data.Rd')
pwd
getdir
getwd()
load('fkd.Rd')
library(doMC)
registerDoMC()
im_rev = matrix(data=im.train[1,], nrow=96, ncol=96)
image(1:96, 1:96, im_rev, col=gray((0:255)/255))
head(im)
im = matrix(data=rev(im.train[1,]), nrow=96, ncol=96)
head(im.train)
im = matrix(data=rev(im.train[1,]), nrow=96, ncol=96)
image(1:96, 1:96, im, col=gray((0:255)/255))
points(96-d.train$nose_tip_x[1],         96-d.train$nose_tip_y[1],         col="red")
points(96-d.train$left_eye_center_x[1],  96-d.train$left_eye_center_y[1],  col="blue")
points(96-d.train$right_eye_center_x[1], 96-d.train$right_eye_center_y[1], col="green")
?points
str(d.train)
str(im.train)
str(im.test)
str(d.test)
head(d.test)
96-d.train$nose_tip_x[i]
96-d.train$nose_tip_x[1]
96-d
96-d.train
96-d.train$nose_tip_x
for(i in 1:nrow(d.train))
{
points(96-d.train$nose_tip_x[i], 96-d.train$nose_tip_y[i], col="red")
}
idx = which.max(d.train$nose_tip_x)
im  = matrix(data=rev(im.train[idx,]), nrow=96, ncol=96)
image(1:96, 1:96, im, col=gray((0:255)/255))
points(96-d.train$nose_tip_x[idx], 96-d.train$nose_tip_y[idx], col="red")
summary(d.train)
setwd('/Users/Copper/Git/DistrictDataLabs/04-team1/Combined')
library(quanteda)
library(stringr)
library(syuzhet)
library(ggplot2)
library(foreign)
library(gistr)
#Load the CSV
setwd('/Users/Copper/Git/DistrictDataLabs/04-team1/Combined')
stories <- textfile("*.txt", full.names = TRUE)
##Create corpus called "speechCorpus"
storyCorpus <- corpus(stories)
#Get Summary of Documents
#This will look a normal structured text document
summary(storyCorpus, n=5)
#Corpus consisting of 239 documents, showing 5 documents
texts(storyCorpus)[2]
texts(storyCorpus)[5]
kwic(storyCorpus, "Trump")
##Measure the readability of the texts
docvars(storyCorpus, "readability")  <- readability(storyCorpus, "Flesch")
docvars(storyCorpus, "readabilityGrade")  <- readability(storyCorpus, "Flesch.Kincaid")
#The first variable is titled "readiability"
#The second variable is titled "readabilityGrade"
#The command itself is "readability()"
#Preview to ensure that the variables have been created
summary(storyCorpus, n=5)
#Create a dataframe
tokenInfo <- summary(storyCorpus)
#Plot the distribution of the readabiality scores
ggplot(tokenInfo, aes(x=readabilityGrade)) + geom_histogram(binwidth=.5, fill="#99badd", colour="black") +   theme(panel.background = element_rect(colour = 'black', size = 1, linetype='solid'))  +  labs(x="Readability Score",y="Frequency")
#Create a "Sentiment Score" for each news story
docvars(storyCorpus, "Sentiment") <- get_sentiment(storyCorpus$documents$texts, method="afinn")
#The command is "get_sentiment()". It comes from the syuzhet package
#Preview to ensure that the variables have been created
summary(storyCorpus, n=5)
#Update dataset to include the new readability grade variables
tokenInfo <- summary(storyCorpus)
###Generate emotional content of speeches
nrc_data <- get_nrc_sentiment(storyCorpus$documents$texts)
names(nrc_data)
View(nrc_data)
docvars(storyCorpus, "anger") <- nrc_data$anger
docvars(storyCorpus, "trust") <- nrc_data$trust
docvars(storyCorpus, "disgust") <- nrc_data$disgust
docvars(storyCorpus, "joy") <- nrc_data$joy
docvars(storyCorpus, "fear") <- nrc_data$fear
docvars(storyCorpus, "sadness") <- nrc_data$sadness
docvars(storyCorpus, "anticipation") <- nrc_data$anticipation
barplot(
sort(colSums(prop.table(nrc_data[, 1:8]))),
horiz = TRUE,
cex.names = 0.7,
las = 1,
main = "Emotions in News Articles", xlab="Percentage"
)
#Plot the distribution of the sentiment scores
ggplot(tokenInfo, aes(x=Sentiment)) + geom_histogram(binwidth=4, fill="#99badd", colour="black") +   theme(panel.background = element_rect(colour = 'black', size = 1, linetype='solid'))  +  labs(x="Sentiment Score",y="Frequency")
###Plotting the Emotions of News Stories
p1 <- ggplot(tokenInfo, aes(x=fear)) + geom_histogram(binwidth=3, fill="#99badd", colour="black") +   theme(panel.background = element_rect(colour = 'black', size = 1, linetype='solid'))  +  labs(x="Sentiment Score",y="Frequency") + expand_limits(x=c(0,50)) + ggtitle("Fear") +  theme(plot.title = element_text(lineheight=.8, face="bold"))
p2 <- ggplot(tokenInfo, aes(x=anger)) + geom_histogram(binwidth=3, fill="#99badd", colour="black") +   theme(panel.background = element_rect(colour = 'black', size = 1, linetype='solid'))  +  labs(x="Sentiment Score",y="Frequency") + expand_limits(x=c(0,50)) + ggtitle("Anger") +  theme(plot.title = element_text(lineheight=.8, face="bold"))
p3 <- ggplot(tokenInfo, aes(x=sadness)) + geom_histogram(binwidth=3, fill="#99badd", colour="black") +   theme(panel.background = element_rect(colour = 'black', size = 1, linetype='solid'))  +  labs(x="Sentiment Score",y="Frequency") + expand_limits(x=c(0,50)) + ggtitle("Sadness") +  theme(plot.title = element_text(lineheight=.8, face="bold"))
p4 <- ggplot(tokenInfo, aes(x=joy)) + geom_histogram(binwidth=3, fill="#99badd", colour="black") +   theme(panel.background = element_rect(colour = 'black', size = 1, linetype='solid'))  +  labs(x="Sentiment Score",y="Frequency") + expand_limits(x=c(0,50)) + ggtitle("Joy") +  theme(plot.title = element_text(lineheight=.8, face="bold"))
#Function defined below...
multiplot(p1, p2, p3, p4, cols=2)
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
library(grid)
# Make a list from the ... arguments and plotlist
plots <- c(list(...), plotlist)
numPlots = length(plots)
# If layout is NULL, then use 'cols' to determine layout
if (is.null(layout)) {
# Make the panel
# ncol: Number of columns of plots
# nrow: Number of rows needed, calculated from # of cols
layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
ncol = cols, nrow = ceiling(numPlots/cols))
}
if (numPlots==1) {
print(plots[[1]])
} else {
# Set up the page
grid.newpage()
pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
# Make each plot, in the correct location
for (i in 1:numPlots) {
# Get the i,j matrix positions of the regions that contain this subplot
matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
layout.pos.col = matchidx$col))
}
}
}
install.packages('quanteda')
